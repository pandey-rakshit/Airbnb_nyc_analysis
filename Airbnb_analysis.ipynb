{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77412171",
   "metadata": {},
   "source": [
    "# Project Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb83ac",
   "metadata": {},
   "source": [
    "**Project Type -** EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9c0a",
   "metadata": {},
   "source": [
    "**Contribution -** Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c6294",
   "metadata": {},
   "source": [
    "**Rakshit Pandey**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5715c3b",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73838af7",
   "metadata": {},
   "source": [
    "**Write the summary here within 500-600 words**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5181ca2",
   "metadata": {},
   "source": [
    "## Github Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedebceb",
   "metadata": {},
   "source": [
    "**Provide your Github Link here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bf9bf",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bfde9",
   "metadata": {},
   "source": [
    "**Write Problem Statement Here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db63441",
   "metadata": {},
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9762f35",
   "metadata": {},
   "source": [
    "**Write Business Context Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660839d6",
   "metadata": {},
   "source": [
    "### Define Your Business Objective ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1812b7",
   "metadata": {},
   "source": [
    "#### Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b6f32",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2ecc9",
   "metadata": {},
   "source": [
    "**Dataset** :  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df01eb",
   "metadata": {},
   "source": [
    "**Data overview** :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02058",
   "metadata": {},
   "source": [
    "## Let's Begin !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417e5f5",
   "metadata": {},
   "source": [
    "### Project Setup -- constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = '' # (support raw files only) - extension - {csv, xlsx, json, parquet}\n",
    "skew_threshold = 0.5\n",
    "outlier_threshold = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1e81",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Installation\n",
    "\n",
    "!pip install scipy scikit-learn pandas numpy matplotlib seaborn wordcloud --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37851b46",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations, including mathematical functions like mean, median, sqrt, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating a wide range of visualizations, such as bar plots, histograms, scatter plots, etc.\n",
    "import seaborn as sns  # For advanced statistical visualizations, including pairplots, violin plots, and heatmaps\n",
    "\n",
    "from scipy import stats  # For statistical functions like boxcox transformation, normality tests, correlation calculations, and more\n",
    "from sklearn.preprocessing import PowerTransformer  # For data scaling and transformations, e.g., Yeo-Johnson for skewed data\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setting styles for plots\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c8457",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26144e4-47ed-4c1a-b83c-f73396a421f3",
   "metadata": {},
   "source": [
    "#### Visualizatioin - Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7a499-dbf8-4e42-8fc5-988fb80d7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart visualzation function\n",
    "\n",
    "plot_functions = {\n",
    "    'scatter': sns.scatterplot,\n",
    "    'line': sns.lineplot,\n",
    "    'bar': sns.barplot,\n",
    "    'hist': sns.histplot,\n",
    "    'pie': plt.pie,\n",
    "    'count': sns.countplot\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_chart(chart_objs, nrows=1, ncols=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a custom visualization chart with optional subplots.\n",
    "\n",
    "    Parameters:\n",
    "    - chart_objs (list): List of dictionaries, where each dictionary contains\n",
    "                          chart-specific information like 'plot_function', 'titles', etc.\n",
    "    - nrows (int): Number of rows for the subplot grid. Default is 1.\n",
    "    - ncols (int): Number of columns for the subplot grid. Default is 1.\n",
    "    - **kwargs: Additional common keyword arguments passed to the plotting function.\n",
    "\n",
    "    Returns:\n",
    "    - fig (matplotlib.figure.Figure): The created figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(ncols * 5.33, nrows * 4))\n",
    "    \n",
    "    # print(axes)\n",
    "    \n",
    "    # Loop through chart_objs to plot the respective data\n",
    "    for i, chart in enumerate(chart_objs, 1):\n",
    "\n",
    "        plot_function = chart['plot_function']\n",
    "        title = chart['title']\n",
    "        xlabel = chart['xlabel']\n",
    "        ylabel = chart['ylabel']\n",
    "        x = chart['x']\n",
    "        y = chart.get('y', None)\n",
    "        chart_kwargs = chart.get('kwargs', {})\n",
    "        \n",
    "        plt.subplot(nrows, ncols, i)\n",
    "        \n",
    "        # Construct the plotting function arguments\n",
    "        plot_args = {'x': x}\n",
    "        if y is not None:\n",
    "            plot_args['y'] = y\n",
    "        \n",
    "        # Add any additional keyword arguments for this specific chart\n",
    "        plot_args.update(chart_kwargs)\n",
    "        \n",
    "        # Plot the chart on the appropriate subplot axis\n",
    "        # sns.histplot(x=x, kde=True, color='purple', element='poly', ax=axes[i], **plot_args)\n",
    "\n",
    "        # Call the plot function\n",
    "        plot_function(**plot_args)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "\n",
    "        \n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f30dfb-2e27-460a-b5ba-0e9273fd3b4c",
   "metadata": {},
   "source": [
    "#### DataFrame - Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b27b48-f8ff-4f1e-ac33-5f717b3ecf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_extension_from_path(input_path):\n",
    "    # Use Path on system path (e.g., relative or absolute)\n",
    "    file_extension = Path(input_path).suffix.lstrip('.')\n",
    "    return file_extension.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe(data_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads raw data from a URL or file path and creates a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_url (str): URL or file path of the raw data (e.g., GitHub raw file).\n",
    "        data_type (str): The type of the data ('csv', 'xlsx', 'json', 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Map data types to pandas functions\n",
    "    read_functions = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"xlsx\": pd.read_excel,\n",
    "        \"json\": pd.read_json,\n",
    "        \"parquet\": pd.read_parquet,\n",
    "    }\n",
    "\n",
    "    data_type = get_file_extension_from_path(data_url)\n",
    "    \n",
    "    if data_type not in read_functions:\n",
    "        raise ValueError(f\"Unsupported data type: {data_type}\")\n",
    "\n",
    "    try:\n",
    "        # Use the appropriate pandas function to read the data\n",
    "        df = read_functions[data_type](data_url)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create DataFrame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d619ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Performs analysis on the dataset and stores the results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        exclude_columns (list): List of columns to exclude from summary statistics (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the results of the analysis.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # General dataset info\n",
    "    results['rows'], results['columns'] = df.shape\n",
    "\n",
    "    # Missing values\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_details = missing_count[missing_count > 0].to_dict()\n",
    "    total_missing = missing_count.sum()\n",
    "    results['missing_values'] = {\n",
    "        'total': total_missing,\n",
    "        'percentage': (total_missing / (results['rows'] * results['columns'])) * 100,\n",
    "        'details': missing_details\n",
    "    }\n",
    "\n",
    "    # Duplicate rows\n",
    "    results['duplicate_rows'] = df.duplicated().sum()\n",
    "\n",
    "    # Data types\n",
    "    results['data_types'] = df.dtypes.reset_index()\n",
    "    results['data_types'].columns = ['Column', 'DataType']\n",
    "\n",
    "    # Exclude columns from analysis (if provided)\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    if exclude_columns:\n",
    "        numeric_df = numeric_df.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "    # Summary statistics\n",
    "    results['statistics'] = numeric_df.describe().T\n",
    "    results['statistics'] = results['statistics'].round(2)  # Format statistics to 2 decimal points\n",
    "\n",
    "    # Additional observations for each column\n",
    "    observations = {}\n",
    "    for col in results['statistics'].index:\n",
    "        stats = results['statistics'].loc[col]\n",
    "        mean = stats['mean']\n",
    "        median = stats['50%']\n",
    "        if mean < median:\n",
    "            observations[col] = \"Data is left-skewed (mean < median).\"\n",
    "        elif mean > median:\n",
    "            observations[col] = \"Data is right-skewed (mean > median).\"\n",
    "        else:\n",
    "            observations[col] = \"Data is symmetric (mean â‰ˆ median).\"\n",
    "\n",
    "    results['observations'] = observations\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce759a-c892-45f6-8111-fe847889a147",
   "metadata": {},
   "source": [
    "#### Skewness Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skewness(df, columns=None):\n",
    "    \"\"\"\n",
    "    Calculate skewness for specific columns or all numeric columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        columns (list): A list of column names for which skewness needs to be calculated. \n",
    "                          If None, skewness will be calculated for all numeric columns.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with column names as keys and skewness values as values.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        # If no column names are provided, calculate skewness for all numeric columns\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    else:\n",
    "        # If specific column names are provided, use those columns\n",
    "        numeric_columns = [col for col in columns if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "    # Calculate skewness for the selected columns\n",
    "    skewness_dict = {col: df[col].skew() for col in numeric_columns}\n",
    "    return skewness_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04e6f7-84b1-4e37-9468-a8a4ebf4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_skewness_with_chart(df, numeric_columns):\n",
    "    \"\"\"\n",
    "    Visualize skewness of all numeric columns using histograms with KDE.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze and visualize.\n",
    "    \n",
    "    Returns:\n",
    "        fig: The generated figure object.\n",
    "    \"\"\"\n",
    "    # Identify numeric columns\n",
    "    # numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Calculate skewness for each numeric column\n",
    "    skewness = df[numeric_columns].skew().round(2)\n",
    "\n",
    "    # print(skewness)\n",
    "    \n",
    "    # Create chart_objs with customized titles and other options for each column\n",
    "    chart_objs = []\n",
    "    for col in numeric_columns:\n",
    "        # print(col, skewness[col])\n",
    "        chart_objs.append({\n",
    "            'plot_function':plot_functions['hist'] ,\n",
    "            'title': f'Skewness of {col}: {skewness[col]}',  # Title for each individual column\n",
    "            'xlabel': col,\n",
    "            'ylabel': 'Frequency',\n",
    "            'x': df[col],\n",
    "            'kwargs': {'kde': True, 'color': 'purple', 'element': 'poly'}\n",
    "        })\n",
    "\n",
    "\n",
    "    # print(chart_objs)\n",
    "    \n",
    "    # Visualize skewness using the visualize_chart function\n",
    "    return visualize_chart(chart_objs, nrows=(len(numeric_columns) // 3 + 1), ncols=3)\n",
    "\n",
    "# # Example: Visualize the skewness of all numeric columns in the DataFrame\n",
    "# fig = visualize_skewness_with_chart(df)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44213569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_skewness(df, columns=None):\n",
    "    \"\"\"\n",
    "    Analyze the skewness of numeric columns in the dataset and generate a summary of skewness categories.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        columns (list, optional): List of columns to include in the skewness analysis. \n",
    "                                   If None, all numeric columns will be considered.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the skewness analysis result categorized by 'high', 'moderate', and 'low'.\n",
    "        plt.Figure: A Matplotlib figure object displaying histograms of the skewness.\n",
    "    \"\"\"\n",
    "    # Determine the columns to analyze (use all numeric columns if no specific columns are provided)\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    \n",
    "    # Check if the provided columns are numeric\n",
    "    non_numeric_columns = [col for col in columns if df[col].dtype not in ['int64', 'float64']]\n",
    "    \n",
    "    if non_numeric_columns:\n",
    "        print(f\"Warning: The following non-numeric columns were excluded from skewness analysis: {', '.join(non_numeric_columns)}\")\n",
    "        # Remove non-numeric columns from the columns list\n",
    "        columns = [col for col in columns if col not in non_numeric_columns]\n",
    "\n",
    "    # If there are no numeric columns to analyze, raise an error\n",
    "    if not columns:\n",
    "        raise ValueError(\"No numeric columns available in the dataset for skewness analysis.\")\n",
    "    \n",
    "    # Calculate skewness for each numeric column\n",
    "    skewness_values = df[columns].apply(lambda x: x.skew()).to_dict()\n",
    "\n",
    "    # Categorize columns based on skewness values\n",
    "    high_skew = [col for col, skew in skewness_values.items() if abs(skew) > 1]\n",
    "    moderate_skew = [col for col, skew in skewness_values.items() if 0.5 < abs(skew) <= 1]\n",
    "    low_skew = [col for col, skew in skewness_values.items() if abs(skew) <= 0.5]\n",
    "\n",
    "    # Prepare the result object\n",
    "    skewness_result = {\n",
    "        \"high_skew\": high_skew,\n",
    "        \"moderate_skew\": moderate_skew,\n",
    "        \"low_skew\": low_skew,\n",
    "        \"skewness_values\": skewness_values  # For detailed skewness values of each column\n",
    "    }\n",
    "\n",
    "    # print(skewness_result)\n",
    "    # Generate and visualize skewness distribution for the specified columns\n",
    "    visualize_skewness_with_chart(df, columns)\n",
    "\n",
    "    return skewness_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06545658-10d4-46b9-8345-3108501840a0",
   "metadata": {},
   "source": [
    "#### DataFrame - Manipulation - Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c4f08-e300-42d7-94ba-ba5325883113",
   "metadata": {},
   "source": [
    "##### Dtype - conversion _(changing column dtype)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df, columns, new_type):\n",
    "    \"\"\"\n",
    "    Converts specified columns to a new data type.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to transform.\n",
    "        columns (list): List of column names to convert.\n",
    "        new_type (type): The new data type (e.g., int, float, str).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated column types.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        try:\n",
    "            df[column] = df[column].astype(new_type)\n",
    "            print(f\"Column '{column}' successfully converted to {new_type}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column '{column}' to {new_type}: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0a8da-e121-4dec-8a27-e3c1c42b07be",
   "metadata": {},
   "source": [
    "##### Column Imputation -- _(Handling missing values)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_column(col, method):\n",
    "    \"\"\"\n",
    "    Impute missing values in a column based on the specified method.\n",
    "    \n",
    "    Args:\n",
    "        col (pd.Series): The column to impute.\n",
    "        method (str): The imputation method ('median', 'mean', or 'mode').\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: The column with imputed values.\n",
    "    \"\"\"\n",
    "    if method == 'median':\n",
    "        return col.fillna(col.median())\n",
    "    elif method == 'mean':\n",
    "        return col.fillna(col.mean())\n",
    "    elif method == 'mode':\n",
    "        return col.fillna(col.mode()[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported imputation method: {method}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Impute missing values for numeric, categorical, datetime, and boolean columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to process.\n",
    "        exclude_columns (list): Columns to exclude from the imputation process.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed missing values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no exclude_columns are provided, initialize as an empty list\n",
    "    exclude_columns = exclude_columns or []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns and df[col].isnull().any():  # Check if the column has missing values\n",
    "            dtype = df[col].dtype\n",
    "\n",
    "            # Handle numeric columns\n",
    "            if dtype in ['int64', 'float64']:\n",
    "                skew = df[col].skew()  # Calculate skewness for numeric columns\n",
    "                if abs(skew) > 0.5:\n",
    "                    df[col] = impute_column(df[col], 'median')\n",
    "                else:\n",
    "                    df[col] = impute_column(df[col], 'mean')\n",
    "            # Handle categorical, boolean, and datetime columns\n",
    "            elif dtype in ['object', 'category', 'bool', 'datetime64[ns]']:\n",
    "                df[col] = impute_column(df[col], 'mode')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefe767-60b7-42cd-9389-2494f810d92b",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers_in_data(df):\n",
    "    \"\"\"\n",
    "    Detect and handle outliers in the DataFrame using the IQR method. This function can remove or transform outliers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check for outliers.\n",
    "        outlier_threshold (float): The IQR threshold for detecting outliers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers handled (removed or transformed).\n",
    "    \"\"\"\n",
    "    # Outlier detection using IQR method\n",
    "    df_no_outliers = df.copy()  # Avoid modifying original DataFrame\n",
    "    \n",
    "    for col in df.select_dtypes(include=[\"number\"]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - outlier_threshold * IQR\n",
    "        upper_bound = Q3 + outlier_threshold * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df_no_outliers[(df_no_outliers[col] < lower_bound) | (df_no_outliers[col] > upper_bound)]\n",
    "        \n",
    "        print(f\"Outliers detected for {col}: {len(outliers)}\")\n",
    "        \n",
    "        # Optionally, you can choose to remove or handle the outliers differently.\n",
    "        df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower_bound) & (df_no_outliers[col] <= upper_bound)]\n",
    "    \n",
    "    return df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4928e-824b-4c8d-a271-83b7ac1ceac0",
   "metadata": {},
   "source": [
    "#### Data Transformation - _(log, sqrt)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b546b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_transformation_with_outliers(df, skew_categories, handle_outliers=False):\n",
    "    \"\"\"\n",
    "    Apply the best transformation based on skewness for each column already categorized into high, moderate, or low skew.\n",
    "    Optionally, handle outliers by removing or transforming them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing numeric columns.\n",
    "        skew_categories (dict): Dictionary with keys 'high', 'moderate', 'low' mapping to lists of column names.\n",
    "        handle_outliers (bool): Whether to handle outliers by removal after transformation.\n",
    "        skew_threshold (float): The threshold for determining the degree of skewness for transformation.\n",
    "        outlier_threshold (float): The threshold for detecting outliers based on IQR method.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with transformations applied to columns and outliers handled (if applicable).\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()  # Create a copy to apply transformations\n",
    "\n",
    "    # Dictionary of transformations for each skew category\n",
    "    transformations = {\n",
    "        'high': 'log',\n",
    "        'moderate': 'sqrt'\n",
    "    }\n",
    "    \n",
    "    # 'low': 'boxcox'  # Optional, or we can skip transformation for low skew\n",
    "\n",
    "    # Apply transformations based on skew category\n",
    "    for skew_category, transformation_type in transformations.items():\n",
    "        columns = skew_categories.get(skew_category, [])\n",
    "        for col in columns:\n",
    "            print(f\"Applying {transformation_type} transformation to {col} due to {skew_category} skewness.\")\n",
    "            transformed_df[col] = apply_transformation(transformed_df, col, transformation_type)\n",
    "    \n",
    "    # Handle outliers if required\n",
    "    if handle_outliers:\n",
    "        transformed_df = handle_outliers_in_data(transformed_df)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687a3a-00d7-47c7-949f-85ab48c81604",
   "metadata": {},
   "source": [
    "#### Summary Generation -- _(Factory class to register and use summary related functions)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc490e-b02f-40b5-94e1-50ffa6776389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Factory Class for Summaries\n",
    "class SummaryFactory:\n",
    "    def __init__(self):\n",
    "        self.steps = {}\n",
    "\n",
    "    def register_step(self, step_name, step_function):\n",
    "        \"\"\"\n",
    "        Registers a summary step to the factory.\n",
    "        \n",
    "        Args:\n",
    "            step_name (str): The name of the step.\n",
    "            step_function (function): The function to generate the summary for this step.\n",
    "        \"\"\"\n",
    "        self.steps[step_name] = step_function\n",
    "\n",
    "    def generate_summary(self, step_name, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates the summary for the specified step.\n",
    "        \n",
    "        Args:\n",
    "            step_name (str): The name of the step to execute.\n",
    "            *args: Positional arguments for the step function.\n",
    "            **kwargs: Keyword arguments for the step function.\n",
    "        \n",
    "        Returns:\n",
    "            str: The generated summary.\n",
    "        \"\"\"\n",
    "        if step_name not in self.steps:\n",
    "            raise ValueError(f\"Step '{step_name}' is not registered in the factory.\")\n",
    "        return self.steps[step_name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65201b-0ef1-4321-b467-e9845e8f562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Overview Function\n",
    "def overview_step(result):\n",
    "    \"\"\"\n",
    "    Generates an overview summary from the dataset analysis results.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): The analysis results dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        str: Overview summary.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # General dataset information\n",
    "    summary.append(f\"The dataset contains {result['rows']} rows and {result['columns']} columns.\\n\")\n",
    "\n",
    "    # Missing values\n",
    "    missing_values = result['missing_values']\n",
    "    summary.append(f\"There are {missing_values['total']} missing values across {len(missing_values['details'])} columns.\")\n",
    "    summary.append(f\"Missing values account for {missing_values['percentage']:.2f}% of the dataset.\")\n",
    "    if missing_values['details']:\n",
    "        summary.append(\"Columns with missing values and their counts:\")\n",
    "        for col, count in missing_values['details'].items():\n",
    "            summary.append(f\"  - {col}: {count} missing values\")\n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Duplicate rows\n",
    "    duplicate_rows = result['duplicate_rows']\n",
    "    if duplicate_rows > 0:\n",
    "        summary.append(f\"There are {duplicate_rows} duplicate rows in the dataset.\")\n",
    "    else:\n",
    "        summary.append(\"There are no duplicate rows in the dataset.\")\n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Data types\n",
    "    summary.append(\"Data Types:\\n\")\n",
    "    data_types = result['data_types']\n",
    "    \n",
    "    summary.append(data_types.to_string(index=False))\n",
    "\n",
    "    # for _, row in data_types.iterrows():\n",
    "    #     summary.append(f\"  - {row['Column']}: {row['DataType']}\")\n",
    "    \n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Summary Statistics\n",
    "    summary.append(\"Summary Statistics:\\n\")\n",
    "    statistics = result['statistics']\n",
    "    summary.append(statistics.to_string())  # Use pandas' `to_string` for a clean table-like output\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebe2f0-201e-4843-9cdb-e6914734255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Observations Function\n",
    "def observations_step(result):\n",
    "    \"\"\"\n",
    "    Generates observations based on numerical analysis.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): The analysis results dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        str: Observations summary.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Observations from numerical columns\n",
    "    summary.append(\"Observations based on the dataset:\\n\")\n",
    "    observations = result.get('observations', {})\n",
    "    \n",
    "    for col, observation in observations.items():\n",
    "        summary.append(f\"  - {col}: {observation}\")\n",
    "\n",
    "    return \"\\n\".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1947d-f0a7-4059-bf66-9577a4f1b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Correlation Matrix Summary (as an example)\n",
    "def correlation_step(result):\n",
    "    \"\"\"\n",
    "    Generates a summary of the correlation matrix.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): The analysis results dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        str: Correlation matrix summary.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    correlation_matrix = result.get('correlation_matrix', None)\n",
    "    if correlation_matrix is not None:\n",
    "        summary.append(\"Correlation Matrix (Top 5 Strongest Correlations):\\n\")\n",
    "        # Extract strongest correlations (ignoring self-correlation)\n",
    "        correlations = correlation_matrix.unstack().reset_index()\n",
    "        correlations.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "        correlations = correlations[correlations['Variable 1'] != correlations['Variable 2']]\n",
    "        correlations = correlations.sort_values(by='Correlation', key=lambda x: x.abs(), ascending=False).head(5)\n",
    "\n",
    "        summary.append(correlations.to_string(index=False))\n",
    "    else:\n",
    "        summary.append(\"Correlation Matrix not available in the result.\")\n",
    "\n",
    "    return \"\\n\".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78673406-4373-4a1a-86ee-347350cc0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness_summary(skewness_object):\n",
    "    \"\"\"\n",
    "    Generate a textual and tabular summary of skewness using the skewness object.\n",
    "\n",
    "    Args:\n",
    "        skewness_object (dict): A dictionary containing skewness analysis results.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted textual and tabular summary of the skewness analysis.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Categorize and format the summary based on the skewness object\n",
    "    if skewness_object.get(\"high_skew\"):\n",
    "        summary.append(f\"- Highly skewed columns (suggesting log transformation): {', '.join(skewness_object['high_skew'])}\")\n",
    "    if skewness_object.get(\"moderate_skew\"):\n",
    "        summary.append(f\"- Moderately skewed columns (suggesting square root transformation): {', '.join(skewness_object['moderate_skew'])}\")\n",
    "    if skewness_object.get(\"low_skew\"):\n",
    "        summary.append(f\"- Columns with low skewness (no transformation needed): {', '.join(skewness_object['low_skew'])}\")\n",
    "    \n",
    "    # Convert detailed skewness values into a table\n",
    "    if skewness_object.get(\"skewness_values\"):\n",
    "        skewness_df = pd.DataFrame.from_dict(skewness_object[\"skewness_values\"], orient=\"index\", columns=[\"Skewness\"])\n",
    "        skewness_df.index.name = \"Column\"\n",
    "        skewness_df.reset_index(inplace=True)\n",
    "        summary.append(\"\\nDetailed Skewness Values:\\n\")\n",
    "        summary.append(skewness_df.to_string(index=False))\n",
    "\n",
    "    return \"\\n\".join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48685681-ddca-4328-a0fe-ed850fa004e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_factory = SummaryFactory()\n",
    "\n",
    "# Register steps\n",
    "summary_factory.register_step(\"overview\", overview_step)\n",
    "summary_factory.register_step(\"observations\", observations_step)\n",
    "summary_factory.register_step(\"correlation\", correlation_step)\n",
    "summary_factory.register_step(\"skewness\", skewness_summary)\n",
    "\n",
    "# print(summary_factory.generate_summary(\"overview\", result)) # to generate summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82951d5f-81ab-42c9-853f-f0715df43e28",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0e07d-5916-4d13-a603-370e32240068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe\n",
    "\n",
    "df = create_dataframe(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609d066-b50f-4daf-a228-a4dab0980230",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = {'id', 'host_id', 'latitude', 'longitude'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f6e39-01ab-4084-8745-b47b4bda6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_dataset(df, exclude_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf28574-1246-4705-b721-acdddc571af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"overview\", result))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(summary_factory.generate_summary(\"observations\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce9b6a-fa30-40eb-8829-bb1dfdc54d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skewness = analyze_skewness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c8f88-b5ae-48b6-8e01-905f2f04384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"skewness\", skewness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a20e4-cd55-4553-bbfa-e8bc4c7c5277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea49a4-350e-45b7-acb2-337c3765955c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(rex)",
   "language": "python",
   "name": "rex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
